###This program performs Ordinary least square regression on a synthetic dataset generated by the Franke Function
###and returns model evalutation (MSE and R2) and optimal predictor
###The loop is on polynomial degree
###Author: R Corseri

import numpy as np
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression, Ridge, Lasso
from sklearn.preprocessing import PolynomialFeatures, StandardScaler
from sklearn.pipeline import Pipeline

from functions import ScaleData, LinReg
from DesignMatrix import DesignMatrix
import FrankeFunction as FF
import Calculate_MSE_R2 as error

#polynomial degree up to 10
maxdegree= 6

# Make data set.
n = 500


x1 = np.random.uniform(0,1,n)
x2 = np.random.uniform(0,1,n)


y = FF.FrankeFunction(x1,x2)#+np.random.normal(0,1,n)


x1 = np.array(x1).reshape(n,1)
x2 = np.array(x2).reshape(n,1)

x = np.hstack((x1,x2)).reshape(n,2)

#Design matrix


#Split train (80%) and test(20%) data before looping on polynomial degree
x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2)

    
#Scaling
#x_train, x_test, y_train, y_test = ScaleData(x_train, x_test, y_train, y_test)
y_train_mean = np.mean(y_train)
x_train_mean = np.mean(x_train)
x_train = x_train - x_train_mean
y_train = y_train - y_train_mean
x_test = x_test - x_train_mean
y_test = y_test - y_train_mean


#Initialization
#Initialize before looping:
TestError = np.zeros(maxdegree+1)
TrainError = np.zeros(maxdegree+1)
TestR2 = np.zeros(maxdegree+1)
TrainR2 = np.zeros(maxdegree+1)
polydegree = np.zeros(maxdegree+1)
predictor = []


#OLS
for degree in range(0,maxdegree+1):
    
    if degree ==0:
        
        Beta = 0
        y_pred = np.mean(y_test)
        y_fit = np.mean(y_train)
        
    
    else:
        X_train = DesignMatrix(x_train[:,0],x_train[:,1],degree)
        X_test = DesignMatrix(x_test[:,0],x_test[:,1],degree)
        y_fit, y_pred, Beta = LinReg(X_train, X_test, y_train, y_test)

    predictor=np.append(predictor,Beta)
    polydegree[degree] = degree
    TestError[degree] = error.MSE(y_test,y_pred)
    TrainError[degree] = error.MSE(y_train,y_fit)
    TestR2[degree] = error.R2(y_test,y_pred)
    TrainR2[degree] = error.R2(y_train,y_fit)
    
    #Display regression results for each polynomial degree
    print("\nModel complexity:")
    print(degree)
    print("\nOptimal estimator Beta")
    print(Beta)
    print("\nTraining error")
    print("MSE =",error.MSE(y_train,y_fit))
    print("R2 =",error.R2(y_train,y_fit))
    print("\nTesting error")
    print("MSE =",error.MSE(y_test,y_pred))
    print("R2  =",error.R2(y_test,y_pred))



#####Plots####
    
#MSE
plt.plot(polydegree, TestError,'r-o' , label='Test MSError')
plt.plot(polydegree, TrainError,'b-o', label='Train MSError')

plt.xlabel("model complexity (degree)")
plt.ylabel("Mean squared error")
plt.legend()
plt.savefig("Results/MSE_vs_complexity.png",dpi=150)
plt.show()

#R2 score
plt.plot(polydegree, TestR2,'r-d', label='Test R2')
plt.plot(polydegree, TrainR2, 'b-d',label='Train R2')
plt.xticks(np.arange(1, 6, step=1))  # Set label locations.
plt.xlabel("model complexity (degree)")
plt.ylabel("R2 score")
plt.legend()
plt.savefig("Results/R2_vs_complexity.png",dpi=150)
plt.show()

#Beta coefficients
print(predictor.shape)

plt.plot(predictor[0],'md-' , label='degree=0')
plt.plot(predictor[1:4],'r-*' , label='degree=1')
plt.plot(predictor[4:10],'b-*' , label='degree=2')
plt.plot(predictor[10:20],'g*-' , label='degree=3')
plt.plot(predictor[20:35],'y*-' , label='degree=4')
plt.plot(predictor[35:56],'k*-' , label='degree=5')


locs, labels = plt.xticks()  # Get the current locations and labels.
plt.xticks(np.arange(0, 1, step=1))  # Set label locations.
plt.xticks(np.arange(21), [r'$\beta_0$', r'$\beta_1$', r'$\beta_2$', \
           r'$\beta_3$', r'$\beta_4$', r'$\beta_5$', \
           r'$\beta_6$', r'$\beta_7$', r'$\beta_8$', \
           r'$\beta_9$', r'$\beta_{10}$', r'$\beta_{11}$', \
           r'$\beta_{12}$', r'$\beta_{13}$', r'$\beta_{14}$', \
           r'$\beta_{15}$', r'$\beta_{16}$', r'$\beta_{17}$', \
           r'$\beta_{18}$', r'$\beta_{19}$', r'$\beta_{20}$',r'$\beta_{21}$'\
           ], rotation=45)  # Set text labels.


plt.ylabel("Optimal Beta - predictor value")
plt.legend()
plt.savefig("Results/Optimal_predictor.png",dpi=150)













