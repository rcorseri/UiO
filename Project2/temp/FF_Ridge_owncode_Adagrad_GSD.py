'''This program performs Ordinary least square regression on a synthetic dataset generated by the Franke Function
and returns model evalutation (MSE and R2) and optimal predictor
The loop is on polynomial degree
Author: R Corseri & L Barreiro'''


import matplotlib.pyplot as plt
import numpy as np
from sklearn.model_selection import train_test_split
#from sklearn.preprocessing import StandardScaler
from Functions import Beta_std, FrankeFunction, R2, MSE, DesignMatrix, LinReg
from Minibatch import create_mini_batches


#OLS on the Franke function
#Create data
#np.random.seed(2003)
n = 100
maxdegree = 5

x = np.random.uniform(0,1,n)
y = np.random.uniform(0,1,n)
z = FrankeFunction(x, y)
# Add random distributed noise
var = 0.1
z = z + np.random.normal(0,var,z.shape)


x = np.array(x).reshape(n,1)
y = np.array(y).reshape(n,1)

x1 = np.hstack((x,y)).reshape(n,2)


# Split the data in test (80%) and training dataset (20%) 
x_train, x_test, z_train, z_test = train_test_split(x1, z, test_size=0.2)

z_train = np.reshape(z_train,(z_train.shape[0],1))
z_test = np.reshape(z_test,(z_test.shape[0],1))



#Plot the resulting scores (MSE and R2) as functions of the polynomial degree (here up to polymial degree five). 

#Initialize before looping:
TestError = np.zeros(maxdegree)
TrainError = np.zeros(maxdegree)
TestR2 = np.zeros(maxdegree)
TrainR2 = np.zeros(maxdegree)
polydegree = np.zeros(maxdegree)
#predictor = []
#predictor_std = []
lmb = 0.0001

for degree in range(maxdegree):
    X_train = DesignMatrix(x_train[:,0],x_train[:,1],degree+1)
    X_test = DesignMatrix(x_test[:,0],x_test[:,1],degree+1)

    #Ridge With gradient descent
    M = 20   #size of each minibatch
    m = int(z.shape[0]/M) #number of minibatches
    n_epochs = 5000 #number of epochs
    
    
    betas = np.random.randn(X_train.shape[1],1)
    eta = 0.0001
    delta = 10**-7
    j = 0
    
    for epoch in range(1,n_epochs+1):
        mini_batches = create_mini_batches(X_train,z_train,M) 
        Giter = np.zeros(shape=(X_train.shape[1],X_train.shape[1]))
        for minibatch in mini_batches:
            X_mini, z_mini = minibatch
            gradients = (2.0/M)*X_mini.T @ (X_mini @ betas - z_mini) + 2*lmb*betas
          
            	# Calculate the outer product of the gradients
            Giter +=gradients @ gradients.T 
            #Simpler algorithm with only diagonal elements
            Ginverse = np.c_[eta/(delta+np.sqrt(np.diagonal(Giter)))]
            # compute update
            update = np.multiply(Ginverse,gradients)
            betas -= update
        j+=1
    
    z_pred = X_test @ betas
    z_fit = X_train @ betas


    polydegree[degree] = degree+1    
    TestError[degree] = MSE(z_test, z_pred)
    TrainError[degree] = MSE(z_train, z_fit)
    TestR2[degree] = R2(z_test,z_pred)
    TrainR2[degree] = R2(z_train,z_fit)
    
    #Display regression results for each polynomial degree
    print("\nModel complexity:")
    print(degree+1)
    print("\nOptimal estimator Beta")
    print(betas.shape)
    print("\nTraining error")
    print("MSE =",MSE(z_train,z_fit))
    print("R2 =",R2(z_train,z_fit))
    print("\nTesting error")
    print("MSE =",MSE(z_test,z_pred))
    print("R2  =",R2(z_test,z_pred))

#Plots 

#MSE   
plt.plot(polydegree, TestError, label='Test sample')
plt.plot(polydegree, TrainError, label='Train sample')
plt.xlabel('Model complexity (degree)')
plt.ylabel('Mean Square Error')
plt.xticks(np.arange(1, maxdegree+1, step=1))  # Set label locations.
plt.legend()
plt.savefig("Results/Ridge/AdagradSGD_MSE_vs_complexity.png", dpi=150)
plt.show()

#R2 score
plt.plot(polydegree, TestR2, label='Test sample')
plt.plot(polydegree, TrainR2, label='Train sample')
plt.xlabel('Model complexity')
plt.ylabel('R2 score')
plt.xticks(np.arange(1, maxdegree+1, step=1))  # Set label locations.
plt.legend()
plt.savefig("Results/Ridge/AdagradSGD_R2_vs_complexity.png", dpi=150)
plt.show()

