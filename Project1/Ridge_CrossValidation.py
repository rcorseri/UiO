###This###This program performs Ridge regression on a synthetic dataset generated by the Franke Function
###with cross-validation resampling
###The program returns plot Error against polynomial degree (up to 10) for a given hyper parameter Lamda
###Author: R Corseri


import numpy as np
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression, Ridge, Lasso
from sklearn.preprocessing import PolynomialFeatures, StandardScaler
from sklearn.pipeline import Pipeline
from sklearn.utils import resample
from sklearn.model_selection import KFold
from functions import LinReg, RidgeReg, LassoReg, MSE, R2, Beta_std
from DesignMatrix import DesignMatrix
from FrankeFunction import FrankeFunction


#Model complexity (polynomial degree up to 7)
maxdegree= 5

#Number of k-fold (between 5 and 10) for cross-validation
k = 10
kfold = KFold(n_splits = k)

#For Ridge regression, set up the hyper-parameters to investigate
nlambdas = 9
lambdas = np.logspace(-4, 4, nlambdas)


# Generate dataset with n observations
n = 100
x1 = np.random.uniform(0,1,n)
x2 = np.random.uniform(0,1,n)

#Define noise
var = 0.01
noise = np.random.normal(0,var,n)

y = FrankeFunction(x1,x2) + noise 
#Add normally distributed noise
#y = y + np.random.normal(0,0.1,y.shape)

x1 = np.array(x1).reshape(n,1)
x2 = np.array(x2).reshape(n,1)
x = np.hstack((x1,x2)).reshape(n,2)

#Scaling
#y_train_mean = np.mean(y_train)
#x_train_mean = np.mean(x_train)
#x_train = x_train - x_train_mean
#y_train = y_train - y_train_mean
#x_test = x_test - x_train_mean
#y_test = y_test - y_train_mean



#Initialize before looping:
polydegree = np.zeros(maxdegree)
error_Kfold = np.zeros((maxdegree,k))
estimated_mse_Kfold = np.zeros(maxdegree)
bias = np.zeros(maxdegree)
variance = np.zeros(maxdegree)


for l in range(nlambdas):   
    i=0
    for degree in range(maxdegree): 
        j=0
        for train_inds, test_inds in kfold.split(x):
            
            x_train = x[train_inds]
            y_train = y[train_inds]   
            x_test = x[test_inds]
            y_test = y[test_inds]
                 
            X_train = DesignMatrix(x_train[:,0],x_train[:,1],degree+1)
            X_test = DesignMatrix(x_test[:,0],x_test[:,1],degree+1)
            y_fit, y_pred, Beta = RidgeReg(X_train, X_test, y_train, y_test,lambdas[l])
            
            error_Kfold[i,j] = MSE(y_test,y_pred)
            
            j+=1
            
        estimated_mse_Kfold[degree] = np.mean(error_Kfold[i,:])
        polydegree[degree] = degree+1
        
        i+=1
     

#####Plots####
    
    plt.plot(polydegree, estimated_mse_Kfold, label='Ridge Error Cross Validation')
    plt.xticks(np.arange(1, len(polydegree)+1, step=1))  # Set label locations.
    plt.xlabel('Model complexity')
    plt.ylabel('Mean squared error')
    plt.title('MSE Ridge regression for lambda = %.0e' %lambdas[l])
    plt.legend()
    plt.savefig("Results/Ridge/Ridge_MSE_lambda=%.0e.png" %lambdas[l],dpi=150)
    plt.show()
        














